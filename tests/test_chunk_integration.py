"""
í…ŒìŠ¤íŠ¸: ì²­í¬ ê°„ ë¶„ë¦¬ëœ í”„ë¡œì„¸ìŠ¤ ì •ì˜ í†µí•©

ì´ í…ŒìŠ¤íŠ¸ëŠ” ë™ì¼í•œ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì •ë³´ê°€ ì—¬ëŸ¬ ì²­í¬ì— ë¶„ì‚°ë˜ì–´ ìˆì„ ë•Œ
ì‹œìŠ¤í…œì´ ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ í†µí•©í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
1. ë™ì¼ í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì–¸ê¸‰ë  ë•Œ í•˜ë‚˜ë¡œ í†µí•©ë˜ëŠ”ì§€
2. ë™ì¼ ì—­í• ì´ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ë°˜ë³µë  ë•Œ í•˜ë‚˜ë¡œ í†µí•©ë˜ëŠ”ì§€
3. ë¶„ë¦¬ëœ íƒœìŠ¤í¬ë“¤ì´ ë™ì¼ í”„ë¡œì„¸ìŠ¤ ë‚´ ìˆœì°¨ì  íë¦„ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€
4. ê²Œì´íŠ¸ì›¨ì´ê°€ í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì˜ ë¶„ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€
"""

import pytest
import sys
import time
from pathlib import Path
from contextlib import contextmanager

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from pdf2bpmn.extractors.entity_extractor import EntityExtractor, ExtractedEntities
from pdf2bpmn.extractors.pdf_extractor import PDFExtractor
from pdf2bpmn.graph.neo4j_client import Neo4jClient
from pdf2bpmn.graph.vector_search import VectorSearch
from pdf2bpmn.workflow.graph import PDF2BPMNWorkflow
from pdf2bpmn.models.entities import generate_id, Process, Task, Role, Gateway, Event
from pdf2bpmn.generators.bpmn_generator import BPMNGenerator


@contextmanager
def timer(name: str):
    """ì‹œê°„ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    start = time.time()
    yield
    elapsed = time.time() - start
    print(f"   â±ï¸ [{name}] {elapsed:.2f}ì´ˆ")


# í…ŒìŠ¤íŠ¸ìš© ì²­í¬ ë°ì´í„° - ì˜ë„ì ìœ¼ë¡œ ë™ì¼ í”„ë¡œì„¸ìŠ¤ ì •ë³´ê°€ ë¶„ì‚°ë¨
CHUNK_1_PROCESS_OVERVIEW = """
ì œ1ì¥ ì´ì¹™

ì œ1ì¡° (ëª©ì )
ì´ ê·œì •ì€ íšŒì‚¬ì˜ êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì— ê´€í•œ ì‚¬í•­ì„ ì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë¶€ì„œë³„ êµ¬ë§¤ ìˆ˜ìš”ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , 
ì˜ˆì‚° ë²”ìœ„ ë‚´ì—ì„œ íš¨ìœ¨ì ì¸ ìì› ë°°ë¶„ì„ ì‹¤í˜„í•˜ê¸° ìœ„í•œ ì—…ë¬´ ì ˆì°¨ì´ë‹¤.

ì œ2ì¡° (ì ìš©ë²”ìœ„)
ì´ ê·œì •ì€ ë³¸ì‚¬ ë° ì§€ì‚¬ì˜ ëª¨ë“  ë¶€ì„œì—ì„œ ë°œìƒí•˜ëŠ” êµ¬ë§¤ìš”ì²­ì— ì ìš©ëœë‹¤.

ì œ3ì¡° (ìš©ì–´ì˜ ì •ì˜)
1. "êµ¬ë§¤ìš”ì²­ì„œ"ë€ ë¬¼í’ˆ ë˜ëŠ” ì„œë¹„ìŠ¤ì˜ êµ¬ë§¤ë¥¼ ìš”ì²­í•˜ëŠ” ê³µì‹ ë¬¸ì„œë¥¼ ë§í•œë‹¤.
2. "ìŠ¹ì¸ê¶Œì"ë€ êµ¬ë§¤ìš”ì²­ì„ ê²€í† í•˜ê³  ìŠ¹ì¸í•  ê¶Œí•œì´ ìˆëŠ” ìë¥¼ ë§í•œë‹¤.
3. "êµ¬ë§¤ë‹´ë‹¹ì"ë€ ìŠ¹ì¸ëœ êµ¬ë§¤ìš”ì²­ì„ ì‹¤ì œë¡œ ì²˜ë¦¬í•˜ëŠ” ë‹´ë‹¹ìë¥¼ ë§í•œë‹¤.

ì œ4ì¡° (êµ¬ë§¤ìš”ì²­ì„œ ì‘ì„±)
êµ¬ë§¤ìš”ì²­ìëŠ” êµ¬ë§¤ê°€ í•„ìš”í•œ ê²½ìš° ì „ì‚°ì‹œìŠ¤í…œì„ í†µí•´ êµ¬ë§¤ìš”ì²­ì„œë¥¼ ì‘ì„±í•œë‹¤.
"""

CHUNK_2_INITIAL_TASKS = """
ì œ6ì¡° (êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„)
êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒì˜ ë‹¨ê³„ë¡œ ì§„í–‰ëœë‹¤.

1ë‹¨ê³„: êµ¬ë§¤ìš”ì²­ì„œ ì ‘ìˆ˜
êµ¬ë§¤ë‹´ë‹¹ìëŠ” ì „ìê²°ì¬ ì‹œìŠ¤í…œì„ í†µí•´ ì ‘ìˆ˜ëœ êµ¬ë§¤ìš”ì²­ì„œë¥¼ í™•ì¸í•œë‹¤.
ì ‘ìˆ˜ëœ ìš”ì²­ì„œëŠ” ìš”ì²­ë²ˆí˜¸ë¥¼ ë¶€ì—¬ë°›ìœ¼ë©°, êµ¬ë§¤ë‹´ë‹¹ìëŠ” í˜•ì‹ì  ìš”ê±´ì„ ê²€í† í•œë‹¤.

2ë‹¨ê³„: ì˜ˆì‚° í™•ì¸
ì¬ë¬´ë‹´ë‹¹ìëŠ” í•´ë‹¹ êµ¬ë§¤ìš”ì²­ì— ëŒ€í•œ ì˜ˆì‚° ê°€ìš© ì—¬ë¶€ë¥¼ í™•ì¸í•œë‹¤.
ì˜ˆì‚°ì´ ë¶€ì¡±í•œ ê²½ìš° ì˜ˆì‚°ì¡°ì • ìš”ì²­ì„ ì§„í–‰í•˜ê±°ë‚˜ êµ¬ë§¤ìš”ì²­ì„ ë°˜ë ¤í•  ìˆ˜ ìˆë‹¤.

3ë‹¨ê³„: ê·œê²© ê²€í† 
ê¸°ìˆ ë‹´ë‹¹ìëŠ” ìš”ì²­ëœ í’ˆëª©ì˜ ê¸°ìˆ ì  ì‚¬ì–‘ê³¼ ê·œê²©ì´ ì ì •í•œì§€ ê²€í† í•œë‹¤.
í•„ìš”ì‹œ ëŒ€ì²´ í’ˆëª©ì´ë‚˜ ìˆ˜ì •ëœ ì‚¬ì–‘ì„ ì œì•ˆí•  ìˆ˜ ìˆë‹¤.
"""

CHUNK_3_ROLE_DEFINITIONS = """
ë³„ì²¨: ì¡°ì§ë„ ë° ì—­í•  ì •ì˜

ì£¼ìš” ì—­í• 

êµ¬ë§¤íŒ€ì¥
- êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ì´ê´„ ì±…ì„ì
- 100ë§Œì› ì´ìƒì˜ êµ¬ë§¤ìš”ì²­ì— ëŒ€í•œ ìµœì¢… ìŠ¹ì¸ ê¶Œí•œ ë³´ìœ 
- ê³µê¸‰ì—…ì²´ ê³„ì•½ ì²´ê²° ê¶Œí•œ

êµ¬ë§¤ë‹´ë‹¹ì
- êµ¬ë§¤ìš”ì²­ì„œ ì ‘ìˆ˜ ë° í˜•ì‹ ê²€í†  ìˆ˜í–‰
- ê²¬ì ì„œ ìˆ˜ì§‘ ë° ë¹„êµë¶„ì„ ë‹´ë‹¹
- ë°œì£¼ì„œ ì‘ì„± ë° ë°œì†¡ ì²˜ë¦¬
- êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ì‹¤ë¬´ ë‹´ë‹¹ì

ì¬ë¬´ë‹´ë‹¹ì
- êµ¬ë§¤ìš”ì²­ì— ëŒ€í•œ ì˜ˆì‚° ê°€ìš© ì—¬ë¶€ í™•ì¸
- ì˜ˆì‚° ì´ˆê³¼ ì‹œ ì˜ˆì‚°ì¡°ì • ìš”ì²­ í”„ë¡œì„¸ìŠ¤ ì§„í–‰
- êµ¬ë§¤ ëŒ€ê¸ˆ ì§€ê¸‰ ìŠ¹ì¸

ê¸°ìˆ ë‹´ë‹¹ì
- ê¸°ìˆ  ì‚¬ì–‘ ë° ê·œê²© ê²€í† 
- ëŒ€ì²´ í’ˆëª© ë˜ëŠ” ì‚¬ì–‘ ë³€ê²½ ì œì•ˆ
- ë‚©í’ˆ ë¬¼í’ˆì˜ ê¸°ìˆ ì  ê²€ìˆ˜
"""

CHUNK_4_LATER_TASKS = """
ì œ12ì¡° (ë°œì£¼ ì²˜ë¦¬)
êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ìŠ¹ì¸ëœ ê±´ì— ëŒ€í•˜ì—¬ êµ¬ë§¤ë‹´ë‹¹ìëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•œë‹¤.

4ë‹¨ê³„: ê²¬ì ì„œ ìˆ˜ì§‘
êµ¬ë§¤ë‹´ë‹¹ìëŠ” 3ê°œ ì´ìƒì˜ ê³µê¸‰ì—…ì²´ë¡œë¶€í„° ê²¬ì ì„œë¥¼ ìˆ˜ì§‘í•œë‹¤.
ê²¬ì ì„œì—ëŠ” í’ˆëª©, ë‹¨ê°€, ë‚©ê¸°, ê²°ì œì¡°ê±´ì´ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

5ë‹¨ê³„: ì—…ì²´ ì„ ì •
ìˆ˜ì§‘ëœ ê²¬ì ì„œë¥¼ ë¹„êµí•˜ì—¬ ìµœì ì˜ ê³µê¸‰ì—…ì²´ë¥¼ ì„ ì •í•œë‹¤.
100ë§Œì› ì´ìƒì˜ ê²½ìš° êµ¬ë§¤íŒ€ì¥ì˜ ìŠ¹ì¸ì„ ë°›ì•„ì•¼ í•œë‹¤.

6ë‹¨ê³„: ë°œì£¼ì„œ ë°œì†¡
ì„ ì •ëœ ì—…ì²´ì— ë°œì£¼ì„œë¥¼ ë°œì†¡í•œë‹¤.
ë°œì£¼ì„œì—ëŠ” í’ˆëª©, ìˆ˜ëŸ‰, ë‹¨ê°€, ë‚©í’ˆì¼, ê²°ì œì¡°ê±´ì„ ëª…ì‹œí•œë‹¤.

ì œ13ì¡° (ì…ê³  ê²€ìˆ˜)
7ë‹¨ê³„: ì…ê³  ê²€ìˆ˜
ë‚©í’ˆëœ ë¬¼í’ˆì— ëŒ€í•´ êµ¬ë§¤ë‹´ë‹¹ìì™€ ê¸°ìˆ ë‹´ë‹¹ìê°€ ê³µë™ìœ¼ë¡œ ê²€ìˆ˜ë¥¼ ì§„í–‰í•œë‹¤.

ì œ14ì¡° (ëŒ€ê¸ˆ ì§€ê¸‰)
8ë‹¨ê³„: ëŒ€ê¸ˆ ì§€ê¸‰
ê²€ìˆ˜ ì™„ë£Œ í›„ ì¬ë¬´ë‹´ë‹¹ìëŠ” ê³µê¸‰ì—…ì²´ì— ëŒ€ê¸ˆì„ ì§€ê¸‰í•œë‹¤.
ëŒ€ê¸ˆ ì§€ê¸‰ì€ êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ìµœì¢… ë‹¨ê³„ì´ë‹¤.
"""

CHUNK_5_GATEWAYS = """
ì œ15ì¡° (ìŠ¹ì¸ ë¶„ê¸° ì¡°ê±´)
êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ìŒì˜ ì¡°ê±´ì— ë”°ë¼ ë¶„ê¸°ê°€ ë°œìƒí•œë‹¤.

ìŠ¹ì¸/ë°˜ë ¤ ë¶„ê¸° (XOR Gateway)
- ì˜ˆì‚° í™•ì¸ ê²°ê³¼ ì˜ˆì‚°ì´ ì¶©ë¶„í•œ ê²½ìš°: ê·œê²© ê²€í†  ë‹¨ê³„ë¡œ ì§„í–‰
- ì˜ˆì‚° í™•ì¸ ê²°ê³¼ ì˜ˆì‚°ì´ ë¶€ì¡±í•œ ê²½ìš°: ì˜ˆì‚°ì¡°ì • ìš”ì²­ ë˜ëŠ” ë°˜ë ¤

ê¸ˆì•¡ë³„ ìŠ¹ì¸ê¶Œì ë¶„ê¸° (XOR Gateway)  
- êµ¬ë§¤ê¸ˆì•¡ì´ 50ë§Œì› ë¯¸ë§Œì¸ ê²½ìš°: êµ¬ë§¤ë‹´ë‹¹ì ìŠ¹ì¸
- êµ¬ë§¤ê¸ˆì•¡ì´ 50ë§Œì› ì´ìƒ 100ë§Œì› ë¯¸ë§Œì¸ ê²½ìš°: êµ¬ë§¤íŒ€ì¥ ìŠ¹ì¸
- êµ¬ë§¤ê¸ˆì•¡ì´ 100ë§Œì› ì´ìƒì¸ ê²½ìš°: ì„ì› ìŠ¹ì¸

ì œ16ì¡° (ê²€ìˆ˜ ë¶„ê¸°)
ê²€ìˆ˜ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° (XOR Gateway)
- ê²€ìˆ˜ í•©ê²©ì¸ ê²½ìš°: ëŒ€ê¸ˆ ì§€ê¸‰ ë‹¨ê³„ë¡œ ì§„í–‰
- ê²€ìˆ˜ ë¶ˆí•©ê²©ì¸ ê²½ìš°: ë°˜í’ˆ ì²˜ë¦¬ ë° ì¬ë°œì£¼ ê²€í† 
"""


class TestChunkIntegration:
    """ì²­í¬ ê°„ í”„ë¡œì„¸ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.neo4j = None
        self.vector_search = None
        self.entity_extractor = None
    
    # í´ë˜ìŠ¤ ë ˆë²¨ ë³€ìˆ˜ - ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” ì—¬ë¶€
    _schema_initialized = False
    
    def setup_method(self):
        """í…ŒìŠ¤íŠ¸ ì „ Neo4j ë°ì´í„° ì´ˆê¸°í™”"""
        print("\nğŸ”§ Setup ì‹œì‘...")
        setup_start = time.time()
        
        with timer("Neo4jClient ìƒì„±"):
            self.neo4j = Neo4jClient()
        
        with timer("VectorSearch ìƒì„±"):
            self.vector_search = VectorSearch(self.neo4j)
        
        with timer("EntityExtractor ìƒì„±"):
            self.entity_extractor = EntityExtractor()
        
        # Neo4j ë°ì´í„° ì „ì²´ ì‚­ì œ (ë¹ ë¥¸ ë°©ì‹)
        with timer("Neo4j ë°ì´í„° ì‚­ì œ"):
            self._clear_neo4j()
        
        # ìŠ¤í‚¤ë§ˆëŠ” í•œ ë²ˆë§Œ ì´ˆê¸°í™” (ì´ë¯¸ ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ)
        if not TestChunkIntegration._schema_initialized:
            with timer("ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”"):
                self._init_schema_once()
            TestChunkIntegration._schema_initialized = True
        
        print(f"   â±ï¸ [Setup ì´ ì‹œê°„] {time.time() - setup_start:.2f}ì´ˆ")
    
    def _clear_neo4j(self):
        """Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì „ì²´ ì´ˆê¸°í™” (ë¹ ë¥¸ ë°©ì‹)"""
        with self.neo4j.session() as session:
            # DETACH DELETEê°€ ë” ë¹ ë¦„
            session.run("MATCH (n) DETACH DELETE n")
        print("   âœ… Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_schema_once(self):
        """ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” (VECTOR INDEX ì œì™¸ - ì§€ì› ì•ˆë˜ëŠ” ë²„ì „ ëŒ€ì‘)"""
        constraints = [
            "CREATE CONSTRAINT doc_id IF NOT EXISTS FOR (d:Document) REQUIRE d.doc_id IS UNIQUE",
            "CREATE CONSTRAINT proc_id IF NOT EXISTS FOR (p:Process) REQUIRE p.proc_id IS UNIQUE",
            "CREATE CONSTRAINT task_id IF NOT EXISTS FOR (t:Task) REQUIRE t.task_id IS UNIQUE",
            "CREATE CONSTRAINT role_id IF NOT EXISTS FOR (r:Role) REQUIRE r.role_id IS UNIQUE",
            "CREATE CONSTRAINT gateway_id IF NOT EXISTS FOR (g:Gateway) REQUIRE g.gateway_id IS UNIQUE",
            "CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:ReferenceChunk) REQUIRE c.chunk_id IS UNIQUE",
        ]
        
        with self.neo4j.session() as session:
            for constraint in constraints:
                try:
                    session.run(constraint)
                except Exception:
                    pass  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ
    
    def test_process_consolidation_across_chunks(self):
        """
        í…ŒìŠ¤íŠ¸ 1: ë™ì¼ í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì–¸ê¸‰ë  ë•Œ í•˜ë‚˜ë¡œ í†µí•©ë˜ëŠ”ì§€
        
        ê¸°ëŒ€ ê²°ê³¼:
        - "êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤"ê°€ CHUNK_1, CHUNK_2, CHUNK_4ì—ì„œ ì–¸ê¸‰ë˜ì§€ë§Œ
        - ìµœì¢…ì ìœ¼ë¡œ í•˜ë‚˜ì˜ Process ì—”í‹°í‹°ë§Œ ìƒì„±ë˜ì–´ì•¼ í•¨
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 1: í”„ë¡œì„¸ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        doc_id = generate_id()
        process_name_to_id = {}
        role_name_to_id = {}
        
        all_processes = []
        
        chunks = [CHUNK_1_PROCESS_OVERVIEW, CHUNK_2_INITIAL_TASKS, CHUNK_4_LATER_TASKS]
        
        for i, chunk in enumerate(chunks):
            chunk_start = time.time()
            print(f"\nğŸ“„ Chunk {i+1} ì²˜ë¦¬ ì¤‘...")
            
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤/ì—­í•  ì´ë¦„ ëª©ë¡ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
            existing_process_names = list(process_name_to_id.keys())
            existing_role_names = list(role_name_to_id.keys())
            
            if existing_process_names:
                print(f"   [ì»¨í…ìŠ¤íŠ¸] ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤: {existing_process_names}")
            
            # ì—”í‹°í‹° ì¶”ì¶œ (ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬) - LLM í˜¸ì¶œ
            with timer("LLM ì¶”ì¶œ"):
                extracted = self.entity_extractor.extract_from_text(
                    chunk,
                    existing_processes=existing_process_names,
                    existing_roles=existing_role_names
                )
            
            # ì—”í‹°í‹° ë³€í™˜ (ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤/ì—­í•  ë§¤í•‘ ì „ë‹¬)
            entities = self.entity_extractor.convert_to_entities(
                extracted,
                doc_id,
                chunk_id=f"chunk_{i+1}",
                existing_processes=process_name_to_id,
                existing_roles=role_name_to_id
            )
            
            # ì¶”ì¶œëœ í”„ë¡œì„¸ìŠ¤ ìˆ˜ì§‘
            for proc in entities["processes"]:
                all_processes.append(proc)
                process_name_to_id[proc.name.lower()] = proc.proc_id
                print(f"   ğŸ†• ìƒˆ í”„ë¡œì„¸ìŠ¤: {proc.name} (ID: {proc.proc_id[:8]}...)")
            
            if not entities["processes"]:
                print(f"   âœ… ìƒˆ í”„ë¡œì„¸ìŠ¤ ì—†ìŒ (ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¬ì‚¬ìš©)")
            
            # ì—­í•  ë§¤í•‘ ì—…ë°ì´íŠ¸
            for role in entities["roles"]:
                role_name_to_id[role.name.lower()] = role.role_id
            
            print(f"   â±ï¸ [Chunk {i+1} ì´] {time.time() - chunk_start:.2f}ì´ˆ")
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ì „ì²´ ì¶”ì¶œëœ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(all_processes)}")
        
        # ì¤‘ë³µ ì œê±° ì‹œë®¬ë ˆì´ì…˜
        unique_process_names = set()
        for proc in all_processes:
            unique_process_names.add(proc.name.lower())
        
        print(f"   ê³ ìœ  í”„ë¡œì„¸ìŠ¤ ì´ë¦„ ìˆ˜: {len(unique_process_names)}")
        print(f"   í”„ë¡œì„¸ìŠ¤ ì´ë¦„ë“¤: {list(unique_process_names)}")
        
        # ê²€ì¦: "êµ¬ë§¤ìš”ì²­ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤" ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ê°€ ìˆëŠ”ì§€
        purchase_process_found = any(
            "êµ¬ë§¤" in name or "ìŠ¹ì¸" in name or "purchase" in name.lower()
            for name in unique_process_names
        )
        
        assert purchase_process_found, "êµ¬ë§¤ ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ê°€ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: í”„ë¡œì„¸ìŠ¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œë¨")
        
        return all_processes, process_name_to_id
    
    def test_role_deduplication_across_chunks(self):
        """
        í…ŒìŠ¤íŠ¸ 2: ë™ì¼ ì—­í• ì´ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ë°˜ë³µë  ë•Œ í•˜ë‚˜ë¡œ í†µí•©ë˜ëŠ”ì§€
        
        ê¸°ëŒ€ ê²°ê³¼:
        - "êµ¬ë§¤ë‹´ë‹¹ì", "ì¬ë¬´ë‹´ë‹¹ì", "ê¸°ìˆ ë‹´ë‹¹ì"ê°€ ì—¬ëŸ¬ ì²­í¬ì—ì„œ ì–¸ê¸‰ë˜ì§€ë§Œ
        - ê°ê° í•˜ë‚˜ì˜ Role ì—”í‹°í‹°ë§Œ ìƒì„±ë˜ì–´ì•¼ í•¨
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 2: ì—­í•  ì¤‘ë³µ ì œê±° í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        doc_id = generate_id()
        process_name_to_id = {}
        role_name_to_id = {}
        
        all_roles = []
        
        # ì—­í• ì´ ë°˜ë³µì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” ì²­í¬ë“¤
        chunks = [CHUNK_2_INITIAL_TASKS, CHUNK_3_ROLE_DEFINITIONS, CHUNK_4_LATER_TASKS]
        
        for i, chunk in enumerate(chunks):
            print(f"\nğŸ“„ Chunk {i+1} ì²˜ë¦¬ ì¤‘...")
            
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤/ì—­í•  ì´ë¦„ ëª©ë¡ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
            existing_process_names = list(process_name_to_id.keys())
            existing_role_names = list(role_name_to_id.keys())
            
            extracted = self.entity_extractor.extract_from_text(
                chunk,
                existing_processes=existing_process_names,
                existing_roles=existing_role_names
            )
            
            entities = self.entity_extractor.convert_to_entities(
                extracted,
                doc_id,
                chunk_id=f"chunk_{i+1}",
                existing_processes=process_name_to_id,
                existing_roles=role_name_to_id
            )
            
            for role in entities["roles"]:
                all_roles.append(role)
                # ê¸°ì¡´ì— ì—†ëŠ” ì—­í• ë§Œ ë§¤í•‘ì— ì¶”ê°€
                if role.name.lower() not in role_name_to_id:
                    role_name_to_id[role.name.lower()] = role.role_id
                    print(f"   ğŸ†• ìƒˆ ì—­í•  ë°œê²¬: {role.name}")
                else:
                    print(f"   â™»ï¸ ê¸°ì¡´ ì—­í•  ì°¸ì¡°: {role.name}")
            
            for proc in entities["processes"]:
                process_name_to_id[proc.name.lower()] = proc.proc_id
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ì „ì²´ ì¶”ì¶œëœ ì—­í•  ìˆ˜: {len(all_roles)}")
        print(f"   ê³ ìœ  ì—­í•  ìˆ˜ (role_name_to_id): {len(role_name_to_id)}")
        print(f"   ì—­í•  ëª©ë¡: {list(role_name_to_id.keys())}")
        
        # í•µì‹¬ ì—­í• ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        expected_roles = ["êµ¬ë§¤ë‹´ë‹¹ì", "ì¬ë¬´ë‹´ë‹¹ì", "ê¸°ìˆ ë‹´ë‹¹ì"]
        found_roles = []
        
        for expected in expected_roles:
            for role_name in role_name_to_id.keys():
                if expected in role_name:
                    found_roles.append(expected)
                    break
        
        print(f"   ë°œê²¬ëœ í•µì‹¬ ì—­í• : {found_roles}")
        
        assert len(found_roles) >= 2, f"í•µì‹¬ ì—­í• ì´ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì§€ ì•ŠìŒ: {found_roles}"
        print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: ì—­í• ì´ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œ ë° í†µí•©ë¨")
        
        return all_roles, role_name_to_id
    
    def test_task_sequence_across_chunks(self):
        """
        í…ŒìŠ¤íŠ¸ 3: ë¶„ë¦¬ëœ íƒœìŠ¤í¬ë“¤ì´ ë™ì¼ í”„ë¡œì„¸ìŠ¤ ë‚´ ìˆœì°¨ì  íë¦„ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€
        
        ê¸°ëŒ€ ê²°ê³¼:
        - 1~3ë‹¨ê³„ (CHUNK_2)ì™€ 4~8ë‹¨ê³„ (CHUNK_4)ì˜ íƒœìŠ¤í¬ë“¤ì´
        - ë™ì¼ í”„ë¡œì„¸ìŠ¤ì— ì†í•˜ê³ , ìˆœì„œ(order)ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ì•¼ í•¨
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 3: íƒœìŠ¤í¬ ìˆœì„œ ì—°ê²° í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        doc_id = generate_id()
        process_name_to_id = {}
        role_name_to_id = {}
        
        all_tasks = []
        all_sequence_flows = []
        
        # í”„ë¡œì„¸ìŠ¤ ì •ì˜ ë¨¼ì € ì¶”ì¶œ
        extracted_overview = self.entity_extractor.extract_from_text(CHUNK_1_PROCESS_OVERVIEW)
        entities_overview = self.entity_extractor.convert_to_entities(
            extracted_overview, doc_id, "chunk_0", {}, {}
        )
        for proc in entities_overview["processes"]:
            process_name_to_id[proc.name.lower()] = proc.proc_id
            print(f"   í”„ë¡œì„¸ìŠ¤ ë°œê²¬: {proc.name}")
        
        # íƒœìŠ¤í¬ê°€ ìˆëŠ” ì²­í¬ë“¤ ì²˜ë¦¬
        chunks = [CHUNK_2_INITIAL_TASKS, CHUNK_4_LATER_TASKS]
        
        for i, chunk in enumerate(chunks):
            print(f"\nğŸ“„ Chunk {i+1} (íƒœìŠ¤í¬) ì²˜ë¦¬ ì¤‘...")
            
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤/ì—­í•  ì´ë¦„ ëª©ë¡ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
            existing_process_names = list(process_name_to_id.keys())
            existing_role_names = list(role_name_to_id.keys())
            
            print(f"   [ì»¨í…ìŠ¤íŠ¸] ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤: {existing_process_names}")
            
            extracted = self.entity_extractor.extract_from_text(
                chunk,
                existing_processes=existing_process_names,
                existing_roles=existing_role_names
            )
            
            entities = self.entity_extractor.convert_to_entities(
                extracted,
                doc_id,
                chunk_id=f"task_chunk_{i+1}",
                existing_processes=process_name_to_id,
                existing_roles=role_name_to_id
            )
            
            for task in entities["tasks"]:
                all_tasks.append(task)
                print(f"   íƒœìŠ¤í¬ ë°œê²¬: {task.name} (order: {task.order}, process_id: {task.process_id[:8] if task.process_id else 'None'}...)")
            
            # ìƒˆë¡œ ì¶”ì¶œëœ í”„ë¡œì„¸ìŠ¤ í™•ì¸
            if entities["processes"]:
                for proc in entities["processes"]:
                    print(f"   âš ï¸ ìƒˆ í”„ë¡œì„¸ìŠ¤ ì¶”ì¶œë¨: {proc.name}")
            else:
                print(f"   âœ… ìƒˆ í”„ë¡œì„¸ìŠ¤ ì—†ìŒ (ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¬ì‚¬ìš©)")
            
            # ì‹œí€€ìŠ¤ í”Œë¡œìš° ìˆ˜ì§‘
            all_sequence_flows.extend(entities.get("sequence_flows", []))
            
            # ì—­í•  ë§¤í•‘ ì—…ë°ì´íŠ¸
            for role in entities["roles"]:
                if role.name.lower() not in role_name_to_id:
                    role_name_to_id[role.name.lower()] = role.role_id
            
            for proc in entities["processes"]:
                process_name_to_id[proc.name.lower()] = proc.proc_id
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ì „ì²´ íƒœìŠ¤í¬ ìˆ˜: {len(all_tasks)}")
        print(f"   ì‹œí€€ìŠ¤ í”Œë¡œìš° ìˆ˜: {len(all_sequence_flows)}")
        
        # íƒœìŠ¤í¬ ì •ë ¬í•˜ì—¬ ìˆœì„œ í™•ì¸
        sorted_tasks = sorted(all_tasks, key=lambda t: t.order if t.order is not None else 999)
        print(f"\n   íƒœìŠ¤í¬ ìˆœì„œ:")
        for task in sorted_tasks:
            print(f"     {task.order}: {task.name}")
        
        # ê²€ì¦: íƒœìŠ¤í¬ê°€ ì¶”ì¶œë˜ì—ˆëŠ”ì§€
        assert len(all_tasks) >= 3, f"ì¶©ë¶„í•œ íƒœìŠ¤í¬ê°€ ì¶”ì¶œë˜ì§€ ì•ŠìŒ: {len(all_tasks)}"
        
        # ê²€ì¦: ëŒ€ë¶€ë¶„ì˜ íƒœìŠ¤í¬ê°€ ë™ì¼ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€
        process_ids = [t.process_id for t in all_tasks if t.process_id]
        if process_ids:
            most_common_proc = max(set(process_ids), key=process_ids.count)
            tasks_in_main_process = sum(1 for pid in process_ids if pid == most_common_proc)
            print(f"\n   ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°ëœ íƒœìŠ¤í¬: {tasks_in_main_process}/{len(all_tasks)}")
        
        print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼: íƒœìŠ¤í¬ê°€ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œë¨")
        
        return all_tasks, all_sequence_flows
    
    def test_gateway_extraction(self):
        """
        í…ŒìŠ¤íŠ¸ 4: ê²Œì´íŠ¸ì›¨ì´ê°€ í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì˜ ë¶„ê¸°ì ìœ¼ë¡œ ì¶”ì¶œë˜ëŠ”ì§€
        
        ê¸°ëŒ€ ê²°ê³¼:
        - "ìŠ¹ì¸/ë°˜ë ¤ ë¶„ê¸°", "ê¸ˆì•¡ë³„ ìŠ¹ì¸ê¶Œì ë¶„ê¸°", "ê²€ìˆ˜ ë¶„ê¸°"ê°€
        - Gateway ì—”í‹°í‹°ë¡œ ì¶”ì¶œë˜ê³ , ì¡°ê±´ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ì•¼ í•¨
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 4: ê²Œì´íŠ¸ì›¨ì´ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        doc_id = generate_id()
        process_name_to_id = {}
        
        # ë¨¼ì € í”„ë¡œì„¸ìŠ¤ ì¶”ì¶œ
        extracted_overview = self.entity_extractor.extract_from_text(CHUNK_1_PROCESS_OVERVIEW)
        entities_overview = self.entity_extractor.convert_to_entities(
            extracted_overview, doc_id, "chunk_0", {}, {}
        )
        for proc in entities_overview["processes"]:
            process_name_to_id[proc.name.lower()] = proc.proc_id
        
        # ê²Œì´íŠ¸ì›¨ì´ ì²­í¬ ì²˜ë¦¬
        print(f"\nğŸ“„ ê²Œì´íŠ¸ì›¨ì´ ì²­í¬ ì²˜ë¦¬ ì¤‘...")
        
        # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì´ë¦„ ëª©ë¡ (ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
        existing_process_names = list(process_name_to_id.keys())
        print(f"   [ì»¨í…ìŠ¤íŠ¸] ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤: {existing_process_names}")
        
        extracted = self.entity_extractor.extract_from_text(
            CHUNK_5_GATEWAYS,
            existing_processes=existing_process_names,
            existing_roles=[]
        )
        
        entities = self.entity_extractor.convert_to_entities(
            extracted,
            doc_id,
            chunk_id="gateway_chunk",
            existing_processes=process_name_to_id,
            existing_roles={}
        )
        
        gateways = entities["gateways"]
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ì¶”ì¶œëœ ê²Œì´íŠ¸ì›¨ì´ ìˆ˜: {len(gateways)}")
        
        for gw in gateways:
            print(f"   ê²Œì´íŠ¸ì›¨ì´: {gw.gateway_type.value}")
            print(f"     ì¡°ê±´: {gw.condition[:50]}..." if len(gw.condition) > 50 else f"     ì¡°ê±´: {gw.condition}")
            print(f"     í”„ë¡œì„¸ìŠ¤ ID: {gw.process_id[:8] if gw.process_id else 'None'}...")
        
        # ê²Œì´íŠ¸ì›¨ì´ê°€ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸ (LLM ì‘ë‹µì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        print(f"\n   ê²Œì´íŠ¸ì›¨ì´ ì¶”ì¶œ ì—¬ë¶€: {'ì„±ê³µ' if gateways else 'ì—†ìŒ (LLM ì‘ë‹µì— ë”°ë¼ ë‹¤ë¦„)'}")
        
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ê²Œì´íŠ¸ì›¨ì´ ì¶”ì¶œ í™•ì¸")
        
        return gateways
    
    def test_full_workflow_integration(self):
        """
        í…ŒìŠ¤íŠ¸ 5: ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸
        
        PDF2BPMNWorkflowë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³ 
        ìµœì¢… ê²°ê³¼ê°€ ì˜¬ë°”ë¥´ê²Œ í†µí•©ë˜ëŠ”ì§€ í™•ì¸
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 5: ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        # ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìŠ¤í‚¤ë§ˆëŠ” ì´ë¯¸ setup_methodì—ì„œ ì´ˆê¸°í™”ë¨)
        workflow = PDF2BPMNWorkflow()
        # workflow.neo4j.init_schema()  # ìŠ¤í‚µ - ì´ë¯¸ ì´ˆê¸°í™”ë¨
        
        doc_id = generate_id()
        
        # ëª¨ë“  ì²­í¬ ìˆœì°¨ ì²˜ë¦¬
        all_chunks = [
            CHUNK_1_PROCESS_OVERVIEW,
            CHUNK_2_INITIAL_TASKS, 
            CHUNK_3_ROLE_DEFINITIONS,
            CHUNK_4_LATER_TASKS,
            CHUNK_5_GATEWAYS
        ]
        
        all_processes = []
        all_tasks = []
        all_roles = []
        all_gateways = []
        all_events = []
        
        for i, chunk in enumerate(all_chunks):
            print(f"\nğŸ“„ Chunk {i+1}/{len(all_chunks)} ì²˜ë¦¬ ì¤‘...")
            
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤/ì—­í•  ì´ë¦„ ëª©ë¡ (ì»¨í…ìŠ¤íŠ¸ ì£¼ì…ìš©)
            existing_process_names = list(workflow.process_name_to_id.keys())
            existing_role_names = list(workflow.role_name_to_id.keys())
            
            if existing_process_names:
                print(f"   [ì»¨í…ìŠ¤íŠ¸] ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤: {existing_process_names}")
            
            extracted = workflow.entity_extractor.extract_from_text(
                chunk,
                existing_processes=existing_process_names,
                existing_roles=existing_role_names
            )
            
            entities = workflow.entity_extractor.convert_to_entities(
                extracted,
                doc_id,
                chunk_id=f"full_chunk_{i+1}",
                existing_processes=workflow.process_name_to_id,
                existing_roles=workflow.role_name_to_id
            )
            
            all_processes.extend(entities["processes"])
            all_tasks.extend(entities["tasks"])
            all_roles.extend(entities["roles"])
            all_gateways.extend(entities["gateways"])
            all_events.extend(entities.get("events", []))
            
            # ì›Œí¬í”Œë¡œìš° ë§¤í•‘ ì—…ë°ì´íŠ¸
            workflow.task_role_map.update(entities.get("task_role_map", {}))
            workflow.task_process_map.update(entities.get("task_process_map", {}))
            workflow.entity_chunk_map.update(entities.get("entity_chunk_map", {}))
            workflow.sequence_flows.extend(entities.get("sequence_flows", []))
            
            for proc in entities["processes"]:
                workflow.process_name_to_id[proc.name.lower()] = proc.proc_id
            for role in entities["roles"]:
                workflow.role_name_to_id[role.name.lower()] = role.role_id
            for task in entities["tasks"]:
                workflow.task_name_to_id[task.name.lower()] = task.task_id
        
        print("\n" + "-"*60)
        print("ğŸ“Š ìµœì¢… í†µí•© ê²°ê³¼:")
        print("-"*60)
        
        print(f"\n[í”„ë¡œì„¸ìŠ¤]")
        print(f"   ì¶”ì¶œëœ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(all_processes)}")
        print(f"   ê³ ìœ  í”„ë¡œì„¸ìŠ¤ ìˆ˜ (name_to_id): {len(workflow.process_name_to_id)}")
        for name, pid in workflow.process_name_to_id.items():
            print(f"     - {name}: {pid[:8]}...")
        
        print(f"\n[ì—­í• ]")
        print(f"   ì¶”ì¶œëœ ì—­í•  ìˆ˜: {len(all_roles)}")
        print(f"   ê³ ìœ  ì—­í•  ìˆ˜ (name_to_id): {len(workflow.role_name_to_id)}")
        for name in list(workflow.role_name_to_id.keys())[:10]:
            print(f"     - {name}")
        
        print(f"\n[íƒœìŠ¤í¬]")
        print(f"   ì¶”ì¶œëœ íƒœìŠ¤í¬ ìˆ˜: {len(all_tasks)}")
        print(f"   ê³ ìœ  íƒœìŠ¤í¬ ìˆ˜ (name_to_id): {len(workflow.task_name_to_id)}")
        
        print(f"\n[ê²Œì´íŠ¸ì›¨ì´]")
        print(f"   ì¶”ì¶œëœ ê²Œì´íŠ¸ì›¨ì´ ìˆ˜: {len(all_gateways)}")
        
        print(f"\n[ê´€ê³„]")
        print(f"   Taskâ†’Role ë§¤í•‘: {len(workflow.task_role_map)}")
        print(f"   Taskâ†’Process ë§¤í•‘: {len(workflow.task_process_map)}")
        print(f"   ì‹œí€€ìŠ¤ í”Œë¡œìš°: {len(workflow.sequence_flows)}")
        
        # ğŸ”— í”„ë¡œì„¸ìŠ¤ ë³‘í•© í…ŒìŠ¤íŠ¸
        print("\n" + "-"*60)
        print("ğŸ”— í”„ë¡œì„¸ìŠ¤ ë³‘í•© í…ŒìŠ¤íŠ¸")
        print("-"*60)
        
        merged_processes, process_id_mapping = workflow._merge_duplicate_processes(all_processes)
        print(f"   ë³‘í•© ì „ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(all_processes)}")
        print(f"   ë³‘í•© í›„ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(merged_processes)}")
        print(f"   ë³‘í•©ëœ í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(all_processes) - len(merged_processes)}")
        
        if process_id_mapping:
            # ë§¤í•‘ì—ì„œ ì‹¤ì œë¡œ ë³‘í•©ëœ ê²ƒë§Œ ì¶œë ¥
            actual_merges = {k: v for k, v in process_id_mapping.items() if k != v}
            if actual_merges:
                print(f"   ID ë§¤í•‘ (ë³‘í•©ë¨): {len(actual_merges)}ê°œ")
        
        # íƒœìŠ¤í¬ process_id ì—…ë°ì´íŠ¸
        updated_tasks = workflow._update_task_process_ids(all_tasks, process_id_mapping)
        
        # ë³‘í•© í›„ í”„ë¡œì„¸ìŠ¤ë³„ íƒœìŠ¤í¬ ìˆ˜ í™•ì¸
        tasks_by_process = {}
        for task in updated_tasks:
            proc_id = task.process_id or "none"
            if proc_id not in tasks_by_process:
                tasks_by_process[proc_id] = []
            tasks_by_process[proc_id].append(task.name)
        
        print(f"\n   í”„ë¡œì„¸ìŠ¤ë³„ íƒœìŠ¤í¬ ë¶„í¬:")
        for proc in merged_processes:
            task_count = len(tasks_by_process.get(proc.proc_id, []))
            print(f"     - {proc.name}: {task_count}ê°œ íƒœìŠ¤í¬")
        
        # Neo4jì— ì €ì¥ (ë³‘í•©ëœ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©)
        print("\nğŸ’¾ Neo4jì— ì €ì¥ ì¤‘...")
        
        for proc in merged_processes:
            try:
                workflow.neo4j.create_process(proc)
            except Exception as e:
                print(f"   í”„ë¡œì„¸ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        for role in all_roles:
            try:
                workflow.neo4j.create_role(role)
            except Exception as e:
                print(f"   ì—­í•  ì €ì¥ ì‹¤íŒ¨: {e}")
        
        for task in all_tasks:
            try:
                workflow.neo4j.create_task(task)
            except Exception as e:
                print(f"   íƒœìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        for gw in all_gateways:
            try:
                workflow.neo4j.create_gateway(gw)
            except Exception as e:
                print(f"   ê²Œì´íŠ¸ì›¨ì´ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ê´€ê³„ ìƒì„±
        try:
            workflow.neo4j.create_all_relationships(
                task_role_map=workflow.task_role_map,
                task_process_map=workflow.task_process_map,
                role_decision_map={},
                entity_chunk_map=workflow.entity_chunk_map
            )
            print("   ê´€ê³„ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"   ê´€ê³„ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ğŸ”— ì‹œí€€ìŠ¤ í”Œë¡œìš° (NEXT ê´€ê³„) ìƒì„±
        print("\nâ¡ï¸ ì‹œí€€ìŠ¤ í”Œë¡œìš° ìƒì„± ì¤‘...")
        print(f"   ìƒì„±í•  ì‹œí€€ìŠ¤ í”Œë¡œìš° ìˆ˜: {len(workflow.sequence_flows)}")
        
        created_flows = 0
        for flow in workflow.sequence_flows:
            try:
                workflow.neo4j.link_task_sequence(
                    flow["from_task_id"],
                    flow["to_task_id"],
                    flow.get("condition", "")
                )
                created_flows += 1
            except Exception as e:
                print(f"   ì‹œí€€ìŠ¤ í”Œë¡œìš° ìƒì„± ì‹¤íŒ¨: {e}")
        
        print(f"   ìƒì„±ëœ ì‹œí€€ìŠ¤ í”Œë¡œìš°: {created_flows}")
        
        # Neo4jì—ì„œ ê²°ê³¼ í™•ì¸
        print("\nğŸ” Neo4j ì €ì¥ ê²°ê³¼ í™•ì¸...")
        
        with workflow.neo4j.session() as session:
            # í”„ë¡œì„¸ìŠ¤ ìˆ˜
            result = session.run("MATCH (p:Process) RETURN count(p) as count")
            process_count = result.single()["count"]
            print(f"   ì €ì¥ëœ í”„ë¡œì„¸ìŠ¤: {process_count}")
            
            # ì—­í•  ìˆ˜
            result = session.run("MATCH (r:Role) RETURN count(r) as count")
            role_count = result.single()["count"]
            print(f"   ì €ì¥ëœ ì—­í• : {role_count}")
            
            # íƒœìŠ¤í¬ ìˆ˜
            result = session.run("MATCH (t:Task) RETURN count(t) as count")
            task_count = result.single()["count"]
            print(f"   ì €ì¥ëœ íƒœìŠ¤í¬: {task_count}")
            
            # ê´€ê³„ ìˆ˜
            result = session.run("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = result.single()["count"]
            print(f"   ì €ì¥ëœ ê´€ê³„: {rel_count}")
            
            # NEXT ê´€ê³„ (ì‹œí€€ìŠ¤ í”Œë¡œìš°) ìˆ˜
            result = session.run("MATCH ()-[r:NEXT]->() RETURN count(r) as count")
            next_count = result.single()["count"]
            print(f"   ì €ì¥ëœ NEXT ê´€ê³„: {next_count}")
        
        # ğŸ”— íƒœìŠ¤í¬ ì‹œí€€ìŠ¤ ê²€ì¦
        print("\n" + "-"*60)
        print("â¡ï¸ íƒœìŠ¤í¬ ì‹œí€€ìŠ¤ ê²€ì¦")
        print("-"*60)
        
        with workflow.neo4j.session() as session:
            # ì‹œí€€ìŠ¤ ì²´ì¸ ì¡°íšŒ
            result = session.run("""
                MATCH path = (t1:Task)-[:NEXT*]->(t2:Task)
                WHERE NOT ()-[:NEXT]->(t1)
                RETURN t1.name as start_task,
                       [node in nodes(path) | node.name] as sequence,
                       length(path) as chain_length
                ORDER BY chain_length DESC
                LIMIT 5
            """)
            
            sequences = list(result)
            if sequences:
                print(f"   ë°œê²¬ëœ ì‹œí€€ìŠ¤ ì²´ì¸: {len(sequences)}ê°œ")
                for seq in sequences:
                    chain = seq["sequence"]
                    print(f"   ğŸ“‹ ì‹œí€€ìŠ¤ ({len(chain)}ê°œ íƒœìŠ¤í¬):")
                    for i, task_name in enumerate(chain):
                        prefix = "   â””â”€" if i == len(chain) - 1 else "   â”œâ”€"
                        print(f"      {prefix} {i+1}. {task_name}")
            else:
                print("   âš ï¸ ì‹œí€€ìŠ¤ ì²´ì¸ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            
            # ê°œë³„ NEXT ê´€ê³„ í™•ì¸
            result = session.run("""
                MATCH (t1:Task)-[r:NEXT]->(t2:Task)
                RETURN t1.name as from_task, t2.name as to_task, r.condition as condition
                ORDER BY t1.name
                LIMIT 15
            """)
            
            next_relations = list(result)
            print(f"\n   NEXT ê´€ê³„ ìƒ˜í”Œ ({len(next_relations)}ê°œ):")
            for rel in next_relations[:10]:
                condition = f" [{rel['condition']}]" if rel['condition'] else ""
                print(f"      {rel['from_task']} â†’ {rel['to_task']}{condition}")
        
        print("\nâœ… ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ê²€ì¦
        assert len(workflow.process_name_to_id) >= 1, "í”„ë¡œì„¸ìŠ¤ê°€ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        assert len(workflow.role_name_to_id) >= 2, "ì—­í• ì´ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        assert len(all_tasks) >= 3, "íƒœìŠ¤í¬ê°€ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        assert len(workflow.sequence_flows) >= 5, "ì‹œí€€ìŠ¤ í”Œë¡œìš°ê°€ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        assert next_count >= 1, "NEXT ê´€ê³„ê°€ Neo4jì— ìƒì„±ë˜ì§€ ì•ŠìŒ"
        
        print(f"\nğŸ“‹ ê²€ì¦ ê²°ê³¼:")
        print(f"   âœ… í”„ë¡œì„¸ìŠ¤: {len(workflow.process_name_to_id)}ê°œ")
        print(f"   âœ… ì—­í• : {len(workflow.role_name_to_id)}ê°œ")
        print(f"   âœ… íƒœìŠ¤í¬: {len(all_tasks)}ê°œ")
        print(f"   âœ… ì‹œí€€ìŠ¤ í”Œë¡œìš°: {len(workflow.sequence_flows)}ê°œ")
        print(f"   âœ… NEXT ê´€ê³„: {next_count}ê°œ")
        
        # ê²°ê³¼ ë°˜í™˜ (BPMN ìƒì„± í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©)
        return {
            "processes": all_processes,
            "roles": all_roles,
            "tasks": all_tasks,
            "gateways": all_gateways,
            "events": all_events,
            "process_name_to_id": workflow.process_name_to_id,
            "role_name_to_id": workflow.role_name_to_id,
            "task_role_map": workflow.task_role_map,
            "sequence_flows": workflow.sequence_flows,
            "neo4j": workflow.neo4j
        }
    
    def test_bpmn_generation_from_graph(self):
        """
        í…ŒìŠ¤íŠ¸ 6: ì™„ì„±ëœ Graphì—ì„œ BPMN XML ìƒì„±
        
        ê¸°ëŒ€ ê²°ê³¼:
        - Neo4jì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ íš¨í•œ BPMN XMLì´ ìƒì„±ë˜ì–´ì•¼ í•¨
        - ëª¨ë“  íƒœìŠ¤í¬ê°€ ì‹œí€€ìŠ¤ í”Œë¡œìš°ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•¨
        """
        print("\n" + "="*60)
        print("í…ŒìŠ¤íŠ¸ 6: BPMN ìƒì„± í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        test_start = time.time()
        
        # í…ŒìŠ¤íŠ¸ 5 ê²°ê³¼ ì¬ì‚¬ìš© ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        workflow_result = self.test_full_workflow_integration()
        
        print("\nğŸ”§ BPMN ìƒì„± ì‹œì‘...")
        
        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
        main_process = None
        for proc in workflow_result["processes"]:
            if "êµ¬ë§¤ìš”ì²­" in proc.name or "ìŠ¹ì¸" in proc.name:
                main_process = proc
                break
        
        if not main_process and workflow_result["processes"]:
            main_process = workflow_result["processes"][0]
        
        if not main_process:
            print("âŒ í”„ë¡œì„¸ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        
        print(f"   ëŒ€ìƒ í”„ë¡œì„¸ìŠ¤: {main_process.name}")
        
        # í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì˜ íƒœìŠ¤í¬ í•„í„°ë§
        process_tasks = [t for t in workflow_result["tasks"] 
                        if t.process_id == main_process.proc_id]
        print(f"   íƒœìŠ¤í¬ ìˆ˜: {len(process_tasks)}ê°œ")
        
        # ê´€ë ¨ ì—­í•  ê°€ì ¸ì˜¤ê¸°
        process_roles = workflow_result["roles"]
        print(f"   ì—­í•  ìˆ˜: {len(process_roles)}ê°œ")
        
        # ê²Œì´íŠ¸ì›¨ì´
        process_gateways = [g for g in workflow_result["gateways"]
                          if g.process_id == main_process.proc_id]
        print(f"   ê²Œì´íŠ¸ì›¨ì´ ìˆ˜: {len(process_gateways)}ê°œ")
        
        # ì´ë²¤íŠ¸
        process_events = workflow_result.get("events", [])
        process_events = [e for e in process_events 
                         if hasattr(e, 'process_id') and e.process_id == main_process.proc_id]
        print(f"   ì´ë²¤íŠ¸ ìˆ˜: {len(process_events)}ê°œ")
        
        # BPMN ìƒì„±
        with timer("BPMN XML ìƒì„±"):
            generator = BPMNGenerator()
            bpmn_xml = generator.generate(
                process=main_process,
                tasks=process_tasks,
                roles=process_roles,
                gateways=process_gateways,
                events=process_events,
                task_role_map=workflow_result["task_role_map"]
            )
        
        # XML ê²€ì¦
        print("\nğŸ“„ ìƒì„±ëœ BPMN XML ê²€ì¦:")
        
        # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
        assert '<?xml version="1.0"' in bpmn_xml, "XML ì„ ì–¸ ì—†ìŒ"
        assert '<bpmn:definitions' in bpmn_xml, "BPMN definitions ì—†ìŒ"
        assert '<bpmn:process' in bpmn_xml, "BPMN process ì—†ìŒ"
        
        # íƒœìŠ¤í¬ í¬í•¨ ì—¬ë¶€ ê²€ì¦
        task_count_in_xml = bpmn_xml.count('<bpmn:userTask') + bpmn_xml.count('<bpmn:serviceTask') + bpmn_xml.count('<bpmn:task ')
        print(f"   XML ë‚´ íƒœìŠ¤í¬: {task_count_in_xml}ê°œ")
        
        # ì‹œí€€ìŠ¤ í”Œë¡œìš° í¬í•¨ ì—¬ë¶€
        flow_count = bpmn_xml.count('<bpmn:sequenceFlow')
        print(f"   XML ë‚´ ì‹œí€€ìŠ¤ í”Œë¡œìš°: {flow_count}ê°œ")
        
        # ë ˆì¸ (ì—­í• ) í¬í•¨ ì—¬ë¶€
        lane_count = bpmn_xml.count('<bpmn:lane')
        print(f"   XML ë‚´ ë ˆì¸: {lane_count}ê°œ")
        
        # íŒŒì¼ë¡œ ì €ì¥
        output_dir = Path(__file__).parent / "output"
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / "test_output.bpmn"
        
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(bpmn_xml)
        
        print(f"\nğŸ’¾ BPMN íŒŒì¼ ì €ì¥: {output_file}")
        
        # XML ì²« ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“ BPMN XML ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 50ì¤„):")
        xml_lines = bpmn_xml.split('\n')[:50]
        for i, line in enumerate(xml_lines, 1):
            print(f"   {i:3d}| {line}")
        
        print(f"\n   ... (ì´ {len(bpmn_xml.split(chr(10)))} ì¤„)")
        
        # ê²€ì¦
        assert task_count_in_xml >= 1, "XMLì— íƒœìŠ¤í¬ê°€ ì—†ìŒ"
        assert flow_count >= 1, "XMLì— ì‹œí€€ìŠ¤ í”Œë¡œìš°ê°€ ì—†ìŒ"
        
        print(f"\nâ±ï¸ [BPMN ìƒì„±] {time.time() - test_start:.2f}ì´ˆ")
        print("\nâœ… BPMN ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        return bpmn_xml


if __name__ == "__main__":
    import sys
    
    total_start = time.time()
    
    # ì§ì ‘ ì‹¤í–‰ ì‹œ
    test = TestChunkIntegration()
    
    # ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¡œ íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰ ê°€ëŠ¥
    # ì˜ˆ: python test_chunk_integration.py 1  -> í…ŒìŠ¤íŠ¸ 1ë§Œ ì‹¤í–‰
    run_only = int(sys.argv[1]) if len(sys.argv) > 1 else None
    
    test_times = {}
    
    try:
        test.setup_method()
        
        tests = [
            ("í…ŒìŠ¤íŠ¸ 1: í”„ë¡œì„¸ìŠ¤ í†µí•©", test.test_process_consolidation_across_chunks),
            ("í…ŒìŠ¤íŠ¸ 2: ì—­í•  ì¤‘ë³µ ì œê±°", test.test_role_deduplication_across_chunks),
            ("í…ŒìŠ¤íŠ¸ 3: íƒœìŠ¤í¬ ìˆœì„œ", test.test_task_sequence_across_chunks),
            ("í…ŒìŠ¤íŠ¸ 4: ê²Œì´íŠ¸ì›¨ì´", test.test_gateway_extraction),
            ("í…ŒìŠ¤íŠ¸ 5: ì „ì²´ ì›Œí¬í”Œë¡œìš°", test.test_full_workflow_integration),
            ("í…ŒìŠ¤íŠ¸ 6: BPMN ìƒì„±", test.test_bpmn_generation_from_graph),
        ]
        
        for i, (name, func) in enumerate(tests, 1):
            if run_only and i != run_only:
                continue
            
            start = time.time()
            func()
            elapsed = time.time() - start
            test_times[name] = elapsed
            print(f"\nâ±ï¸ [{name}] ì´ ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
        print("\n" + "="*60)
        print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*60)
        
        print("\nğŸ“Š í…ŒìŠ¤íŠ¸ë³„ ì†Œìš”ì‹œê°„:")
        for name, elapsed in test_times.items():
            print(f"   {name}: {elapsed:.2f}ì´ˆ")
        
        print(f"\nâ±ï¸ ì „ì²´ ì†Œìš”ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

